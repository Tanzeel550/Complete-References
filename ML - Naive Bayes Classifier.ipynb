{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Installed\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem\n",
    "Naive Bayes classifiers are a collection of classification algorithms based on Bayes’ Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.<br>\n",
    "Bayes’ Theorem finds the probability of an event occurring given the probability of another event that has already occurred. Bayes’ theorem is stated mathematically as the following equation:<br>\n",
    "<img src=\"Images/naivebayes-1.png\">\n",
    "where A and B are events and P(B) ? 0.<br>\n",
    "\n",
    "Basically, we are trying to find probability of event A, given the event B is true. Event B is also termed as evidence.\n",
    "P(A) is the priori of A (the prior probability, i.e. Probability of event before evidence is seen). The evidence is an attribute value of an unknown instance(here, it is event B).<br>\n",
    "\n",
    "P(A|B) is a posteriori probability of B, i.e. probability of event after evidence is seen.<br>\n",
    "Now, with regards to our dataset, we can apply Bayes’ theorem in following way:<br>\n",
    "<img src=\"Images/naivebayes-2.png\">\n",
    "where, y is class variable and X is a dependent feature vector (of size n) where: <br>\n",
    "<img src=\"Images/naivebayes-3.png\">\n",
    "\n",
    "# Naive assumption\n",
    "Now, its time to put a naive assumption to the Bayes’ theorem, which is, independence among the features. So now, we split evidence into the independent parts.<br>\n",
    "\n",
    "Now, if any two events A and B are independent, then,<br>\n",
    "\n",
    "<b>P(A,B) = P(A)P(B)</b>\n",
    "Hence, we reach to the result:<br>\n",
    "<img src=\"Images/naivebayes-4.png\">\n",
    "which can be expressed as:\n",
    "<img src=\"Images/naivebayes-5.png\">\n",
    "Now, as the denominator remains constant for a given input, we can remove that term:\n",
    "<img src=\"Images/naivebayes-6.png\">\n",
    "Now, we need to create a classifier model. For this, we find the probability of given set of inputs for all possible values of the class variable y and pick up the output with maximum probability. This can be expressed mathematically as:\n",
    "<img src=\"Images/naivebayes-7.png\">\n",
    "So, finally, we are left with the task of calculating P(y) and P(xi | y).<br>\n",
    "\n",
    "Please note that P(y) is also called class probability and P(xi | y) is called conditional probability.<br>\n",
    "\n",
    "The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of P(xi | y).<br>\n",
    "\n",
    "# Types of Naive Bayes\n",
    "\n",
    "## 1) Gaussian Naive Bayes\n",
    "In Gaussian Naive Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution. A Gaussian distribution is also called Normal distribution. When plotted, it gives a bell shaped curve which is symmetric about the mean of the feature values as shown below:\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/naive-bayes-classification-1.png\" width=30% height=30%>\n",
    "The likelihood of the features is assumed to be Gaussian, hence, conditional probability is given by:\n",
    "<img src=\"https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-7fb78d7323fcbade0cb664161a8e84c4_l3.svg\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting independent(x) and dependent(y) features from our data\n",
    "x = pd.DataFrame(load_iris()['data'], columns=load_iris()['feature_names'])\n",
    "y = load_iris()['target']\n",
    "\n",
    "# getting our train & test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "gnb = GaussianNB() \n",
    "gnb.fit(x_train, y_train) \n",
    "  \n",
    "# making predictions on the testing set \n",
    "pred = gnb.predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'priors': None, 'var_smoothing': 1e-09}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [0, 1, 0, ], [1, 0, 0, ], [0, 0, 1, ], [0, 1, 0, ], [0, 1, 0, ], [1, 0, 0, ], [0, 1, 0, ], [0, 0, 1, ], [0, 1, 0, ], [0, 1, 0, ], [0, 0, 1, ], [1, 0, 0, ], [1, 0, 0, ], [1, 0, 0, ], [1, 0, 0, ], [0, 0, 1, ], [0, 0, 1, ], [0, 1, 0, ], [0, 1, 0, ], [0, 0, 1, ], [1, 0, 0, ], [0, 0, 1, ], [1, 0, 0, ], [0, 0, 1, ], [0, 0, 1, ], [0, 0, 1, ], [0, 0, 1, ], [0, 0, 1, ], [1, 0, 0, ], [1, 0, 0, ], [1, 0, 0, ], [1, 0, 0, ], [0, 1, 0, ], [1, 0, 0, ], [1, 0, 0, ], [0, 0, 1, ], [0, 1, 0, ], [1, 0, 0, ], [1, 0, 0, ], [1, 0, 0, ], [0, 0, 1, ], [0, 1, 0, ], [0, 1, 0, ], [1, 0, 0, ], [1, 0, 0, ], [0, 1, 0, ], [0, 1, 0, ], [0, 0, 1, ], [0, 1, 0, ], [0, 0, 1, ], ]\n"
     ]
    }
   ],
   "source": [
    "# getting our prediction in some readable format\n",
    "print('[',end=\" \")\n",
    "for i in gnb.predict_proba(x_test):\n",
    "    print('[',end='')\n",
    "    for j in i:\n",
    "        print(\"{0:.0f},\".format(j),end=' ') \n",
    "    print('],',end=\" \")\n",
    "print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  0,  0],\n",
       "       [ 0, 14,  1],\n",
       "       [ 0,  1, 15]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "<ul>\n",
    "    <li>It is not only a simple approach but also a fast and accurate method for prediction.</li>\n",
    "    <li>Naive Bayes has very low computation cost.</li>\n",
    "    <li>It can efficiently work on a large dataset.</li>\n",
    "    <li>It performs well in case of discrete response variable compared to the continuous variable.</li>\n",
    "    <li>It can be used with multiple class prediction problems.</li>\n",
    "    <li>It also performs well in the case of text analytics problems.</li>\n",
    "    <li>When the assumption of independence holds, a Naive Bayes classifier performs better compared to other models like logistic regression.</li>\n",
    "</ul>\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "<li>The assumption of independent features. In practice, it is almost impossible that model will get a set of predictors which are entirely independent.</li>\n",
    "<li>If there is no training tuple of a particular class, this causes zero posterior probability. In this case, the model is unable to make predictions. This problem is known as Zero Probability/Frequency Problem.</li>\n",
    "\n",
    "### When to use Naive Bayes?\n",
    "Naive  Bayes  classifiers  tend  to  perform  especially  well  in  one  of  the  following situations: <br>\n",
    "<ol>\n",
    "    <li>When the naive assumptions actually match the data (very rare in practice)</li>\n",
    "    <li>For very well-separated categories, when model complexity is less important</li>\n",
    "    <li>For very high-dimensional data, when model complexity is less important</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
